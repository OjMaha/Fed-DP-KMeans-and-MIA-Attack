{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Point Reconstruction Attack & Utility Analysis\n",
    "\n",
    "This notebook simulates a data-point reconstruction attack against a federated k-means algorithm, comparing a **non-private** model to a **differentially private (DP)** one.\n",
    "\n",
    "The simulation is broken down into these steps:\n",
    "1.  **Setup:** Import libraries and define all helper functions from the original script.\n",
    "2.  **Baseline Utility:** Train one full non-private model and one full private model (on all data) to measure their baseline k-means cost and accuracy. This demonstrates the **privacy-utility tradeoff**.\n",
    "3.  **Load Attack Data:** Load the federated dataset that the attacker will use to select targets.\n",
    "4.  **Run Attack:** For `NUM_ATTACKS` iterations:\n",
    "    a. Pick a random client and a random data point from their dataset.\n",
    "    b. **Train a model *without* that single point** (once for private, once for non-private). This creates the `centers_baseline`.\n",
    "    c. Simulate the client's update *with* the point (`noisy_update_in`).\n",
    "    d. Simulate the client's update *without* the point (`noisy_update_out`).\n",
    "    e. The reconstructed point is `(noisy_update_in - noisy_update_out)`.\n",
    "    f. Compare the reconstruction to the true point using **L2 Error** and **Cosine Similarity**.\n",
    "5.  **Final Report:** Display the utility metrics and the attack metrics side-by-side.\n",
    "6.  **Cleanup:** Remove all generated config and results files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'fedp (Python 3.10.18)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# --- 1. Setup & Imports ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "import random\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import sys\n",
    "import math\n",
    "import pickle \n",
    "import argparse # For Namespace object\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Add project root to path to ensure imports work ---\n",
    "# This notebook assumes it is in 'reconstruction_attacks/'\n",
    "# To run from 'fed-dp-kmeans-main/', change PROJECT_ROOT to \".\"\n",
    "PROJECT_ROOT = \"..\" \n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    # Find the absolute path of the project root relative to this notebook\n",
    "    # This is a bit more robust for notebooks\n",
    "    try:\n",
    "        notebook_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        notebook_dir = os.path.abspath('')\n",
    "    \n",
    "    project_root_path = os.path.abspath(os.path.join(notebook_dir, PROJECT_ROOT))\n",
    "    if project_root_path not in sys.path:\n",
    "        print(f\"Adding {project_root_path} to system path.\")\n",
    "        sys.path.insert(0, project_root_path)\n",
    "\n",
    "# --- Import from project files ---\n",
    "try:\n",
    "    from data import make_data, set_data_args, add_data_arguments\n",
    "    from utils import kmeans_cost, add_utils_arguments, set_seed, make_results_path\n",
    "    from utils.argument_parsing import maybe_inject_arguments_from_config\n",
    "    from pfl.data.sampling import get_user_sampler\n",
    "    from pfl.data.federated_dataset import FederatedDataset\n",
    "    from pfl.stats import MappedVectorStatistics\n",
    "    from privacy.utils import get_mechanism\n",
    "    from algorithms import add_algorithms_arguments\n",
    "except ImportError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure this notebook is in the 'reconstruction_attacks' directory,\")\n",
    "    print(\"or change the PROJECT_ROOT variable to point to the 'fed-dp-kmeans-main' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Configuration ---\n",
    "\n",
    "# Number of attack iterations (target data points).\n",
    "NUM_ATTACKS = 2\n",
    "\n",
    "# Global random seed for reproducibility.\n",
    "SEED = 42\n",
    "\n",
    "# Base config file to use for DP settings.\n",
    "BASE_CONFIG_PATH = 'configs/gaussians_data_privacy.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recon_configs_datapoint(base_config_path='configs/gaussians_data_privacy.yaml'):\n",
    "    \"\"\"\n",
    "    Creates two config files:\n",
    "    1.  non_private: All privacy mechanisms AND datapoint_privacy flag are False.\n",
    "    2.  private: datapoint_privacy is True and all mechanisms are True.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(base_config_path, 'r') as f: base_config = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Base config file '{base_config_path}' not found. Using defaults.\")\n",
    "        base_config = {'dataset': 'GaussianMixtureUniform', 'K': 10, 'dim': 100,'num_train_clients': 100, 'samples_per_client': 1000,'samples_per_mixture_server': 20, 'num_uniform_server': 100,'initialization_algorithm': 'FederatedClusterInitExact', 'clustering_algorithm': 'FederatedLloyds','minimum_server_point_weight': 5, 'fedlloyds_num_iterations': 1,'datapoint_privacy': True, 'outer_product_epsilon': 1, 'weighting_epsilon': 1,'center_init_gaussian_epsilon': 1, 'center_init_epsilon_split': 0.5,'fedlloyds_epsilon': 1, 'fedlloyds_epsilon_split': 0.5,'outer_product_clipping_bound': 11, 'weighting_clipping_bound': 1,'center_init_clipping_bound': 11, 'center_init_laplace_clipping_bound': 1,'fedlloyds_clipping_bound': 11, 'fedlloyds_laplace_clipping_bound': 1,'overall_target_delta': 1e-6, 'fedlloyds_delta': 1e-6, 'send_sums_and_counts': True}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading base config '{base_config_path}': {e}. Using defaults.\")\n",
    "        base_config = {'dataset': 'GaussianMixtureUniform', 'K': 10, 'dim': 100, 'num_train_clients': 100, 'samples_per_client': 1000,'samples_per_mixture_server': 20, 'num_uniform_server': 100,'initialization_algorithm': 'FederatedClusterInitExact','clustering_algorithm': 'FederatedLloyds', 'minimum_server_point_weight': 5, 'fedlloyds_num_iterations': 1,'datapoint_privacy': True, 'outer_product_epsilon': 1, 'weighting_epsilon': 1, 'center_init_gaussian_epsilon': 1,'center_init_epsilon_split': 0.5, 'fedlloyds_epsilon': 1, 'fedlloyds_epsilon_split': 0.5,'outer_product_clipping_bound': 11, 'weighting_clipping_bound': 1, 'center_init_clipping_bound': 11,'center_init_laplace_clipping_bound': 1, 'fedlloyds_clipping_bound': 11, 'fedlloyds_laplace_clipping_bound': 1,'overall_target_delta': 1e-6, 'fedlloyds_delta': 1e-6, 'send_sums_and_counts': True}\n",
    "    \n",
    "    # Set defaults for required keys\n",
    "    base_config.setdefault('fedlloyds_num_iterations', 1); base_config.setdefault('fedlloyds_cohort_size', base_config.get('num_train_clients', 100))\n",
    "    base_config.setdefault('num_train_clients', 100); base_config.setdefault('send_sums_and_counts', True)\n",
    "    default_delta = base_config.get('overall_target_delta', 1e-6)\n",
    "    base_config.setdefault('overall_target_delta', default_delta); base_config.setdefault('fedlloyds_delta', default_delta)\n",
    "    base_config.setdefault('fedlloyds_clipping_bound', 11); base_config.setdefault('fedlloyds_laplace_clipping_bound', 1)\n",
    "\n",
    "    os.makedirs(\"reconstruction_attacks/configs\", exist_ok=True)\n",
    "    \n",
    "    # --- Config 1: Non-Private (All privacy OFF) ---\n",
    "    config_non_private = base_config.copy()\n",
    "    config_non_private.update({\n",
    "        'datapoint_privacy': False,  # <-- As requested\n",
    "        'outer_product_privacy': False, \n",
    "        'point_weighting_privacy': False,\n",
    "        'center_init_privacy': False, \n",
    "        'fedlloyds_privacy': False,\n",
    "        'fedlloyds_num_iterations': 1\n",
    "    })\n",
    "    config_non_private_fname = 'reconstruction_attacks/configs/gaussian_datapoint_non_private.yaml'\n",
    "    with open(config_non_private_fname, 'w') as f: yaml.dump(config_non_private, f, sort_keys=False)\n",
    "\n",
    "    # --- Config 2: Private (All privacy ON) ---\n",
    "    config_private = base_config.copy()\n",
    "    config_private.update({\n",
    "        'datapoint_privacy': True, # <-- As requested\n",
    "        'outer_product_privacy': True, \n",
    "        'point_weighting_privacy': True,\n",
    "        'center_init_privacy': True, \n",
    "        'fedlloyds_privacy': True,\n",
    "        'fedlloyds_num_iterations': 1\n",
    "    })\n",
    "    config_private.setdefault('fedlloyds_clipping_bound', 11); config_private.setdefault('fedlloyds_laplace_clipping_bound', 1)\n",
    "    config_private.setdefault('fedlloyds_delta', config_private.get('overall_target_delta', 1e-6))\n",
    "    config_private_fname = 'reconstruction_attacks/configs/gaussian_datapoint_private.yaml'\n",
    "    with open(config_private_fname, 'w') as f: yaml.dump(config_private, f, sort_keys=False)\n",
    "    \n",
    "    return config_non_private_fname, config_private_fname\n",
    "\n",
    "# --- get_target_data ---\n",
    "def get_target_data(target_client_id_str, all_train_clients):\n",
    "    user_sampler = get_user_sampler('minimize_reuse', [target_client_id_str])\n",
    "    target_dataset = FederatedDataset(all_train_clients.make_dataset_fn, user_sampler)\n",
    "    (user_dataset, _) = next(target_dataset.get_cohort(1))\n",
    "    if hasattr(user_dataset.raw_data[0], 'numpy'): return user_dataset.raw_data[0].numpy()\n",
    "    return user_dataset.raw_data[0]\n",
    "\n",
    "# --- run_training_get_centers (For the attack) ---\n",
    "def run_training_get_centers(config_file, exclude_client_id_str=None, exclude_datapoint_str=None, seed=None):\n",
    "    \"\"\"Runs run.py for the attack (supports exclusions)\"\"\"\n",
    "    cmd = ['python', 'run.py', '--args_config', config_file]\n",
    "    if exclude_client_id_str: cmd.extend(['--exclude_client_id_str', exclude_client_id_str])\n",
    "    if exclude_datapoint_str: cmd.extend(['--exclude_datapoint', exclude_datapoint_str])\n",
    "    if seed is not None: cmd.extend(['--seed', str(seed)])\n",
    "    print(f\"\\nRunning command: {' '.join(cmd)}\")\n",
    "    try: subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    except subprocess.CalledProcessError as e: print(f\"Error running training: {e}\"); return None\n",
    "    except FileNotFoundError: print(\"Error: 'python' command not found.\"); return None\n",
    "    center_file = 'final_centers.npy'\n",
    "    if not os.path.exists(center_file): print(f\"Error: Center file '{center_file}' not found.\"); return None\n",
    "    return center_file\n",
    "\n",
    "# --- run_main_training_for_utility (For baseline utility) ---\n",
    "def run_main_training_for_utility(config_file, seed=None):\n",
    "    \"\"\"Runs run.py *without* exclusions to generate baseline utility metrics.\"\"\"\n",
    "    cmd = ['python', 'run.py', '--args_config', config_file]\n",
    "    if seed is not None: cmd.extend(['--seed', str(seed)])\n",
    "    print(f\"\\nRunning command for baseline utility: {' '.join(cmd)}\")\n",
    "    try: subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    except subprocess.CalledProcessError as e: print(f\"Error running baseline utility training: {e}\"); return False\n",
    "    except FileNotFoundError: print(\"Error: 'python' command not found.\"); return False\n",
    "    return True\n",
    "\n",
    "# --- get_baseline_utility ---\n",
    "def get_baseline_utility(config_file, seed=None):\n",
    "    \"\"\"\n",
    "    Runs main training once and fetches the cost/accuracy from its summary file.\n",
    "    \"\"\"\n",
    "    # 1. Run main training\n",
    "    success = run_main_training_for_utility(config_file, seed=seed)\n",
    "    if not success:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # 2. Load the config to find the results path\n",
    "    try:\n",
    "        config_namespace = load_config_as_namespace(config_file) # Use existing loader\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading config {config_file} to get results path: {e}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # 3. Load the results file\n",
    "    model_cost, model_accuracy = np.nan, np.nan\n",
    "    try:\n",
    "        # Determine correct path based on the config's privacy flag\n",
    "        privacy_type = 'data_point_level' if config_namespace.datapoint_privacy else 'client_level'\n",
    "        results_path = make_results_path(privacy_type, config_namespace.dataset)\n",
    "        summary_file = os.path.join(results_path, 'summary_results.pkl')\n",
    "        \n",
    "        if os.path.exists(summary_file):\n",
    "            with open(summary_file, 'rb') as f:\n",
    "                results_dict = pickle.load(f)\n",
    "            \n",
    "            # Find the final step's results\n",
    "            final_results_key = 'Clustering' if 'Clustering' in results_dict else 'Initialization'\n",
    "            if final_results_key in results_dict:\n",
    "                model_cost = results_dict[final_results_key].get('Train client cost', np.nan)\n",
    "                model_accuracy = results_dict[final_results_key].get('Train client accuracy', np.nan)\n",
    "        else:\n",
    "            print(f\"  > Warning: Could not find summary file for utility: {summary_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  > Error loading model metrics: {e}\")\n",
    "        \n",
    "    return model_cost, model_accuracy\n",
    "\n",
    "# --- load_config_as_namespace (Loads config + defaults) ---\n",
    "def load_config_as_namespace(config_file):\n",
    "    # (ensures DP defaults)\n",
    "    with open(config_file, 'r') as f: config_dict = yaml.safe_load(f)\n",
    "    config_dict.setdefault('num_train_clients', 100); config_dict.setdefault('send_sums_and_counts', True)\n",
    "    config_dict.setdefault('center_init_send_sums_and_counts', False); \n",
    "    # datapoint_privacy default will be set by the config file itself\n",
    "    config_dict.setdefault('datapoint_privacy', False) # Default to False if not present\n",
    "    default_delta = config_dict.get('overall_target_delta', 1e-6); config_dict.setdefault('overall_target_delta', default_delta)\n",
    "    config_dict.setdefault('fedlloyds_num_iterations', 1); config_dict.setdefault('fedlloyds_cohort_size', config_dict.get('num_train_clients', 100))\n",
    "    config_dict.setdefault('fedlloyds_epsilon', 1.0); config_dict.setdefault('fedlloyds_epsilon_split', 0.5)\n",
    "    config_dict.setdefault('fedlloyds_delta', default_delta); config_dict.setdefault('fedlloyds_clipping_bound', 11)\n",
    "    config_dict.setdefault('fedlloyds_laplace_clipping_bound', 1)\n",
    "    config_dict.setdefault('outer_product_epsilon', 1.0); config_dict.setdefault('outer_product_delta', default_delta)\n",
    "    config_dict.setdefault('outer_product_clipping_bound', 11); config_dict.setdefault('weighting_epsilon', 1.0)\n",
    "    config_dict.setdefault('weighting_clipping_bound', 1); config_dict.setdefault('center_init_gaussian_epsilon', 1.0)\n",
    "    config_dict.setdefault('center_init_delta', default_delta); config_dict.setdefault('center_init_epsilon_split', 0.5)\n",
    "    config_dict.setdefault('center_init_clipping_bound', 11); config_dict.setdefault('center_init_laplace_clipping_bound', 1)\n",
    "    # Add dataset for results path\n",
    "    config_dict.setdefault('dataset', 'GaussianMixtureUniform')\n",
    "    return argparse.Namespace(**config_dict)\n",
    "\n",
    "# --- simulate_client_contribution (Simulates noisy update) ---\n",
    "def simulate_client_contribution(client_data, global_centers, config_namespace, seed=None):\n",
    "    K = global_centers.shape[0]; dim = global_centers.shape[1]\n",
    "    if client_data.shape[0] == 0:\n",
    "        raw_sums = np.zeros((K, dim), dtype=np.float32); raw_counts = np.zeros(K, dtype=np.float32)\n",
    "    else:\n",
    "        if client_data.dtype != global_centers.dtype:\n",
    "             try: client_data = client_data.astype(global_centers.dtype)\n",
    "             except ValueError: print(\"Warning: Data type mismatch, casting to float32.\"); client_data = client_data.astype(np.float32); global_centers = global_centers.astype(np.float32)\n",
    "        dist_matrix = pairwise_distances(client_data, global_centers); assignments = np.argmin(dist_matrix, axis=1)\n",
    "        raw_sums = np.zeros((K, dim), dtype=np.float32); raw_counts = np.zeros(K, dtype=np.float32)\n",
    "        for k in range(K):\n",
    "            mask = (assignments == k)\n",
    "            if np.any(mask): raw_sums[k] = np.sum(client_data[mask], axis=0); raw_counts[k] = np.sum(mask)\n",
    "    raw_stats_dict = {}\n",
    "    if config_namespace.send_sums_and_counts: raw_stats_dict['sum_points_per_component'] = raw_sums; raw_stats_dict['num_points_per_component'] = raw_counts\n",
    "    else: raise NotImplementedError(\"Only supports 'send_sums_and_counts=True'\")\n",
    "    raw_stats = MappedVectorStatistics(raw_stats_dict)\n",
    "    mechanism_name = 'fedlloyds' if config_namespace.fedlloyds_privacy else 'no_privacy'\n",
    "    try:\n",
    "        if not hasattr(config_namespace, 'fedlloyds_delta'): config_namespace.fedlloyds_delta = config_namespace.overall_target_delta\n",
    "        mechanism_wrapper = get_mechanism(config_namespace, mechanism_name); underlying_mechanism = mechanism_wrapper.underlying_mechanism\n",
    "    except Exception as e: print(f\"Error getting mechanism '{mechanism_name}': {e}\"); raise\n",
    "    try: sim_seed_clip = seed + 1 if seed is not None else None; clipped_stats, _ = underlying_mechanism.constrain_sensitivity(raw_stats, seed=sim_seed_clip)\n",
    "    except Exception as e: print(f\"Error during constrain_sensitivity: {e}\"); clipped_stats = raw_stats\n",
    "    try: sim_seed_noise = seed + 2 if seed is not None else None; noisy_stats, _ = underlying_mechanism.add_noise(clipped_stats, cohort_size=1, seed=sim_seed_noise)\n",
    "    except Exception as e: print(f\"Error during add_noise: {e}\"); noisy_stats = clipped_stats\n",
    "    final_noisy_sums = np.zeros_like(raw_sums); final_noisy_counts = np.zeros_like(raw_counts)\n",
    "    if config_namespace.send_sums_and_counts:\n",
    "        if 'sum_points_per_component' in noisy_stats: final_noisy_sums = noisy_stats['sum_points_per_component']\n",
    "        else: print(\"Warning: 'sum_points_per_component' missing.\")\n",
    "        if 'num_points_per_component' in noisy_stats: final_noisy_counts = noisy_stats['num_points_per_component']\n",
    "        else: print(\"Warning: 'num_points_per_component' missing.\")\n",
    "    else: raise NotImplementedError(\"Only supports 'send_sums_and_counts=True'\")\n",
    "    final_noisy_counts = np.maximum(0, final_noisy_counts); return final_noisy_sums, final_noisy_counts\n",
    "\n",
    "def run_reconstruction_once_single_point(config_file, target_client_id_str, target_sample_idx,\n",
    "                                         target_datapoint_vector, full_client_data, seed=None):\n",
    "    \"\"\"\n",
    "    Runs one iteration of the single data point reconstruction attack.\n",
    "    Uses baseline centers trained *without* the target point.\n",
    "    Returns (squared L2 error, cosine similarity).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config_namespace = load_config_as_namespace(config_file)\n",
    "        if not config_namespace.send_sums_and_counts:\n",
    "            print(f\"Error: Config {config_file} has send_sums_and_counts=False. Skipping.\"); return np.nan, np.nan\n",
    "    except Exception as e: print(f\"Error loading config {config_file}: {e}\"); return np.nan, np.nan\n",
    "\n",
    "    # 1. Get Global Centers (Trained WITHOUT the target data point)\n",
    "    print(\"Running training WITHOUT target POINT to get baseline global centers...\")\n",
    "    exclude_str = f\"{target_client_id_str}:{target_sample_idx}\"\n",
    "    train_seed = seed + 50 if seed is not None else None # Seed for training run\n",
    "    centers_file = run_training_get_centers(config_file, exclude_datapoint_str=exclude_str, seed=train_seed)\n",
    "\n",
    "    if centers_file is None: \n",
    "        print(\"Failed to get baseline global centers. Skipping.\"); \n",
    "        return np.nan, np.nan\n",
    "\n",
    "    try: global_centers_baseline = np.load(centers_file) \n",
    "    except Exception as e: \n",
    "        print(f\"Error loading centers {centers_file}: {e}\"); error = np.nan\n",
    "    finally:\n",
    "        if os.path.exists(centers_file): os.remove(centers_file)\n",
    "        if 'error' in locals(): return error, np.nan\n",
    "\n",
    "    # 2. Simulate client contribution WITH the target point (using baseline centers)\n",
    "    print(f\"Simulating contribution WITH target point {target_client_id_str}:{target_sample_idx}...\")\n",
    "    sim_seed_in = seed + 101 if seed is not None else None\n",
    "    try:\n",
    "        noisy_sums_in, noisy_counts_in = simulate_client_contribution(\n",
    "            full_client_data, global_centers_baseline, config_namespace, seed=sim_seed_in # Use baseline centers\n",
    "        )\n",
    "    except Exception as e: \n",
    "        print(f\"Error simulating IN contribution: {e}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # 3. Simulate client contribution WITHOUT the target point (using baseline centers)\n",
    "    print(f\"Simulating contribution WITHOUT target point {target_client_id_str}:{target_sample_idx}...\")\n",
    "    client_data_out = np.delete(full_client_data, target_sample_idx, axis=0)\n",
    "    sim_seed_out = seed + 102 if seed is not None else None\n",
    "    try:\n",
    "        noisy_sums_out, noisy_counts_out = simulate_client_contribution(\n",
    "            client_data_out, global_centers_baseline, config_namespace, seed=sim_seed_out # Use baseline centers\n",
    "        )\n",
    "    except Exception as e: \n",
    "        print(f\"Error simulating OUT contribution: {e}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # 4. Calculate the difference (noisy contribution of the single point)\n",
    "    diff_sums = noisy_sums_in - noisy_sums_out\n",
    "    diff_counts = noisy_counts_in - noisy_counts_out\n",
    "\n",
    "    # 5. Reconstruct: Find cluster k* with max count difference, use corresponding sum difference\n",
    "    if diff_counts.size == 0:\n",
    "        print(\"Error: diff_counts is empty.\"); reconstructed_point = np.full(target_datapoint_vector.shape[1], np.nan)\n",
    "    else:\n",
    "        tie_breaker = np.random.randn(*diff_counts.shape) * 1e-9\n",
    "        probable_cluster_idx = np.argmax(diff_counts + tie_breaker)\n",
    "        reconstructed_point = diff_sums[probable_cluster_idx]\n",
    "\n",
    "    # 6. Calculate Squared L2 Error and Cosine Similarity\n",
    "    error = np.nan\n",
    "    cosine_sim = np.nan # Initialize\n",
    "    if np.isnan(reconstructed_point).any():\n",
    "        print(f\"Target {target_client_id_str}:{target_sample_idx}: Reconstruction failed (NaN).\")\n",
    "    else:\n",
    "        # target_datapoint_vector is (1, D), reconstructed_point is (D,)\n",
    "        target_vec_1d = target_datapoint_vector.squeeze()\n",
    "        error = np.sum((target_vec_1d - reconstructed_point)**2) \n",
    "        \n",
    "        # --- ADDED: Cosine Similarity ---\n",
    "        norm1 = np.linalg.norm(target_vec_1d)\n",
    "        norm2 = np.linalg.norm(reconstructed_point)\n",
    "        \n",
    "        if norm1 > 1e-9 and norm2 > 1e-9:\n",
    "             cosine_sim = np.dot(target_vec_1d, reconstructed_point) / (norm1 * norm2)\n",
    "        elif norm1 < 1e-9 and norm2 < 1e-9:\n",
    "             cosine_sim = 1.0 # Both vectors are zero\n",
    "        else:\n",
    "             cosine_sim = 0.0 # One vector is zero\n",
    "        # --- END ADDED ---\n",
    "\n",
    "        print(f\"Target {target_client_id_str}:{target_sample_idx}: True Norm={norm1:.4f}, Recon Norm={norm2:.4f}, Sq L2 Error={error:.6f}, Cosine Sim={cosine_sim:.6f}\")\n",
    "\n",
    "    return error, cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialization\n",
    "\n",
    "First, we set the global seed and create the two `.yaml` config files that `run.py` will use.\n",
    "-   `gaussian_datapoint_non_private.yaml`: `datapoint_privacy: False` and all sub-mechanisms `False`.\n",
    "-   `gaussian_datapoint_private.yaml`: `datapoint_privacy: True` and all sub-mechanisms `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SEED is not None: \n",
    "    set_seed(SEED)\n",
    "    print(f\"Set global seed to {SEED}\")\n",
    "\n",
    "print(\"--- 1. Creating Single-Point Reconstruction Attack config files ---\")\n",
    "config_non_private_file, config_private_file = create_recon_configs_datapoint(BASE_CONFIG_PATH)\n",
    "print(f\"Non-private config: {config_non_private_file}\")\n",
    "print(f\"Private config: {config_private_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Baseline Model Utility (Privacy-Utility Tradeoff)\n",
    "\n",
    "This is the step you requested. We run the *full* training process (with no data points excluded) once for the non-private config and once for the private config. We then load the `summary_results.pkl` file generated by `run.py` to get the final k-means cost and accuracy for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"--- 2. Calculating Baseline Model Utility (this may take a moment) ---\")\n",
    "utility_metrics = {}\n",
    "\n",
    "print(\"  Running NON-PRIVATE model...\")\n",
    "cost_np, acc_np = get_baseline_utility(config_non_private_file, seed=SEED)\n",
    "utility_metrics['non_private'] = {'cost': cost_np, 'accuracy': acc_np}\n",
    "\n",
    "print(\"  Running PRIVATE model...\")\n",
    "cost_p, acc_p = get_baseline_utility(config_private_file, seed=SEED)\n",
    "utility_metrics['private'] = {'cost': cost_p, 'accuracy': acc_p}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Privacy vs. Utility\n",
    "\n",
    "Here are the baseline utility metrics. This table shows the *privacy-utility tradeoff*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- BASELINE PRIVACY-UTILITY TRADEOFF ---\")\n",
    "print(f\"  NON-PRIVATE: Cost = {cost_np:.6f}, Accuracy = {acc_np:.6f}\")\n",
    "print(f\"  PRIVATE:     Cost = {cost_p:.6f}, Accuracy = {acc_p:.6f}\")\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Client Data for Attack\n",
    "\n",
    "Now, we load the full federated dataset to get the list of clients we can attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 3. Loading client list for Attack ---\")\n",
    "\n",
    "# We need to spoof command-line args for the data loader\n",
    "# This is a bit of a hack required because the project's data loader\n",
    "# is tied to the command-line parsing system.\n",
    "original_argv = sys.argv.copy()\n",
    "try:\n",
    "    sys.argv = ['run.py', '--args_config', config_non_private_file]\n",
    "    temp_parser = argparse.ArgumentParser(add_help=False)\n",
    "    temp_parser = add_data_arguments(temp_parser); temp_parser = add_utils_arguments(temp_parser); temp_parser = add_algorithms_arguments(temp_parser)\n",
    "    maybe_inject_arguments_from_config()\n",
    "    data_args, _ = temp_parser.parse_known_args()\n",
    "    if SEED is not None: data_args.data_seed = SEED\n",
    "    set_data_args(data_args)\n",
    "    all_train_clients, _, _, _ = make_data(data_args)\n",
    "    all_client_ids = [str(i) for i in range(data_args.num_train_clients)]\n",
    "    print(f\"Loaded {len(all_client_ids)} total clients.\")\n",
    "    if not all_client_ids: print(\"Error: No clients loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Fatal Error loading data: {e}\")\n",
    "    print(\"Data Args:\", data_args)\n",
    "finally:\n",
    "    # Restore original argv\n",
    "    sys.argv = original_argv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run the Reconstruction Attack\n",
    "\n",
    "This is the main attack loop. We will iterate `NUM_ATTACKS` times, picking a new random data point each time and attempting to reconstruct it under both private and non-private models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- 4. Starting Reconstruction Attack for {NUM_ATTACKS} points ---\")\n",
    "\n",
    "results = {\n",
    "    'non_private': {'errors': [], 'cosine_sims': []},\n",
    "    'private': {'errors': [], 'cosine_sims': []}\n",
    "}\n",
    "config_files = {'non_private': config_non_private_file, 'private': config_private_file}\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "attacks_run = 0; client_sample_attempts = 0; max_client_attempts = NUM_ATTACKS * 5\n",
    "\n",
    "if 'all_client_ids' in locals() and all_client_ids:\n",
    "    while attacks_run < NUM_ATTACKS and client_sample_attempts < max_client_attempts:\n",
    "        client_sample_attempts += 1\n",
    "        target_client_id = rng.choice(all_client_ids)\n",
    "        print(f\"\\n--- Trying Client {target_client_id} (Attempt {client_sample_attempts}) ---\")\n",
    "        try: full_client_data = get_target_data(target_client_id, all_train_clients)\n",
    "        except StopIteration: print(f\"Error sampling client {target_client_id}.\"); continue\n",
    "        except Exception as e: print(f\"Error loading data for {target_client_id}: {e}\"); continue\n",
    "        if full_client_data.shape[0] == 0: print(f\"Client {target_client_id} has no data.\"); continue\n",
    "\n",
    "        target_sample_idx = rng.integers(0, full_client_data.shape[0])\n",
    "        target_datapoint_vector = full_client_data[target_sample_idx:target_sample_idx+1]\n",
    "        print(f\"Target point: Client {target_client_id}, Index {target_sample_idx} (of {full_client_data.shape[0]})\")\n",
    "\n",
    "        attack_successful_this_iter = False; current_iter_results = {}\n",
    "        for mode in ['non_private', 'private']:\n",
    "            print(f\"--- Running {mode.upper()} scenario ---\")\n",
    "            config_file = config_files[mode]\n",
    "            iter_seed = (SEED + attacks_run*10 + (0 if mode == 'non_private' else 1)) if SEED is not None else None\n",
    "            try:\n",
    "                #Call the reconstruction function\n",
    "                error, cos_sim = run_reconstruction_once_single_point(\n",
    "                    config_file, target_client_id, target_sample_idx,\n",
    "                    target_datapoint_vector, full_client_data, seed=iter_seed\n",
    "                )\n",
    "                if not np.isnan(error):\n",
    "                    current_iter_results[mode] = (error, cos_sim) # Store both\n",
    "                    attack_successful_this_iter = True\n",
    "                else: print(f\"Reconstruction failed in {mode} mode (NaN error).\")\n",
    "            except Exception as e: print(f\"Attack failed unexpectedly for {mode}: {e}\"); import traceback; traceback.print_exc()\n",
    "\n",
    "        # Update results collection\n",
    "        if 'non_private' in current_iter_results and 'private' in current_iter_results:\n",
    "             err_np, sim_np = current_iter_results['non_private']\n",
    "             results['non_private']['errors'].append(err_np)\n",
    "             results['non_private']['cosine_sims'].append(sim_np)\n",
    "             \n",
    "             err_p, sim_p = current_iter_results['private']\n",
    "             results['private']['errors'].append(err_p)\n",
    "             results['private']['cosine_sims'].append(sim_p)\n",
    "             \n",
    "             attacks_run += 1\n",
    "        elif attack_successful_this_iter: print(\"Attack completed for only one mode, results discarded.\")\n",
    "\n",
    "    if attacks_run < NUM_ATTACKS: print(f\"\\nWarning: Only completed {attacks_run}/{NUM_ATTACKS} iterations.\")\n",
    "else:\n",
    "    print(\"\\n--- SKIPPING ATTACK: Client data was not loaded successfully ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Final Results & Analysis\n",
    "\n",
    "Here is the final report. \n",
    "\n",
    "We can see the **Model Utility** (Cost/Accuracy) at the top, followed by the **Reconstruction Attack Metrics** (L2 Error/Cosine Similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- FINAL SINGLE-POINT RECONSTRUCTION RESULTS ---\")\n",
    "\n",
    "print(\"\\n--- Model Utility (Privacy-Utility Tradeoff) ---\")\n",
    "print(f\"  NON-PRIVATE: Cost = {utility_metrics['non_private']['cost']:.6f}, Accuracy = {utility_metrics['non_private']['accuracy']:.6f}\")\n",
    "print(f\"  PRIVATE:     Cost = {utility_metrics['private']['cost']:.6f}, Accuracy = {utility_metrics['private']['accuracy']:.6f}\")\n",
    "\n",
    "print(\"\\n--- Reconstruction Attack Metrics ---\")\n",
    "for mode in ['non_private', 'private']:\n",
    "    res = results[mode]\n",
    "    count = len(res['errors'])\n",
    "    if count > 0:\n",
    "        print(f\"\\n{mode.upper()} Model Results ({count} points):\")\n",
    "        \n",
    "        # Reconstruction Error\n",
    "        errors = res['errors']\n",
    "        avg_error=np.nanmean(errors); std_error=np.nanstd(errors); median_error=np.nanmedian(errors)\n",
    "        print(f\"  Reconstruction Sq L2 Error:\")\n",
    "        print(f\"    Avg: {avg_error:.6f}, StdDev: {std_error:.6f}, Median: {median_error:.6f}\")\n",
    "\n",
    "        # Cosine Similarity\n",
    "        sims = res['cosine_sims']\n",
    "        avg_sim=np.nanmean(sims); std_sim=np.nanstd(sims); median_sim=np.nanmedian(sims)\n",
    "        print(f\"  Reconstruction Cosine Similarity:\")\n",
    "        print(f\"    Avg: {avg_sim:.6f}, StdDev: {std_sim:.6f}, Median: {median_sim:.6f}\")\n",
    "    else: \n",
    "        print(f\"\\n{mode.upper()} Model: No successful attacks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Cleanup\n",
    "\n",
    "Finally, we remove all the temporary config files, model center files (`.npy`), and results files (`.pkl`) created during the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 6. Cleaning up generated files... ---\")\n",
    "try:\n",
    "    if os.path.exists(config_non_private_file): \n",
    "        os.remove(config_non_private_file)\n",
    "        print(f\"Removed {config_non_private_file}\")\n",
    "    if os.path.exists(config_private_file): \n",
    "        os.remove(config_private_file)\n",
    "        print(f\"Removed {config_private_file}\")\n",
    "    if os.path.exists('final_centers.npy'): \n",
    "        os.remove('final_centers.npy')\n",
    "        print(\"Removed final_centers.npy\")\n",
    "        \n",
    "    # Clean up utility results files\n",
    "    for cfg_file in [config_non_private_file, config_private_file]:\n",
    "        try:\n",
    "            ns = load_config_as_namespace(cfg_file) # Config file might be deleted, load from memory\n",
    "            ptype = 'data_point_level' if ns.datapoint_privacy else 'client_level'\n",
    "            rpath = make_results_path(ptype, ns.dataset)\n",
    "            sfile = os.path.join(rpath, 'summary_results.pkl')\n",
    "            if os.path.exists(sfile): \n",
    "                os.remove(sfile)\n",
    "                print(f\"Removed {sfile}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Note: Could not clean up summary file for {cfg_file}. It may already be gone. {e}\")\n",
    "            \n",
    "    print(\"Cleanup complete.\")\n",
    "except OSError as e: \n",
    "    print(f\"Error during cleanup: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
